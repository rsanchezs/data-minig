---
output:
  word_document:
    highlight: zenburn
    reference_docx: word-styles-reference-33.docx
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Format d´entrega 

Aquest document s´ha realitzat mitjançant __Markdown__[^1] amb l´ajuda de l´entorn de desenvolupament __RStudio__[^2] utilitzant les característiques que aquest ofereix per
a la creació de documents __R__ reproduïbles.

La documentació generada en la realització de la pràctica  es troba allotjada en __GitHub__ al següent repositori:

* https://github.com/rsanchezs/data-minig

En aquest repositori es poden trobar els següents fitxers:

* Aquest document en formats __pdf__ i __docx__ amb el nom rsanchezs_PAC2.
* Un document __R Markdown__[^3] que es pot utilitzar per a reproduir tots els exemples
presentats a la PAC.
* El conjunt de dades utilitzades.



![](img/propietat-intelectual.PNG)



[^1]: https://es.wikipedia.org/wiki/Markdown
[^2]: https://www.rstudio.com/
[^3]: https://rmarkdown.rstudio.com/



# Exercici 1

En relació amb el cas pràctic que vaig desenvolupar a la PAC1 es tractava d´implementar un _recommender system_ (de l´anglès, sistemes de recomanació).
És per això que, utilitzar els mètodes no supervistats no seria un bona elecció.

Avui dia, l´algoritme _Nearest Neighborhood_ (de l´anglès, algoritme del veí més proper), és el més utilitzat avui dia en els sistemes de recomanació.

Els algoritmes de veïns més pròxims s´han desenvolupat en dues perspectives possibles: recomanació de veïns pròxims per usuari i per ítem:

* **Centrats en usuari _(User kNN)_:** Es recomanen a l'usuari ítems que han agradat a usuaris similars.

* **Centrats en ítem _(Item kNN)_:** Es recomanen a l'usuari ítems que es pareixen
a ítems que li han agradat.



# Exercici 2


## Requisits

Per començar, per a la realització del nostre anàlisi necessitarem els següents 
paquets:

* `cluster` per a la computació dels algoritmes d´agregació.
* `factoextra` per a la visualització de resultats d´agregació i que es fonamenta
en el paquet `ggplot2`.[^1]
* `clValid` que s´utilitza per a comparar els mètodes d´agregació.

El paquet `factoextra` conté funcions per anàlisi de _clustering_ i visualització dels
resultats:

| Funció | Descripció |
|---|---|
|`dist(fviz_dist, get_dist)`  | Visualització i computació de la matriu de distàncies |
| `get_clust_tendency` | Avaluació de la tendència d´agregació |
| `fviz_nbclust(fviz_gap_stat)` | Determinació del nombre òptim de clústers |
| `fviz_dend` | Visualització de dendrogrames |
| `fviz_cluster` | Visualització dels resultats d´agrupament |
| `fviz_mclust` | Visualització dels resultats del model d´agrupament |
| `fviz_silhouette` | Visualització de la informació de la silueta |
| `hkmeans` | K-means jeràrquic |
| `eclust` | Visualització de l´anàlisi de agrupament |


Podem instal·lar els dos paquets com es mostra en la següent línia de codi:

```{r eval=FALSE}
# Instalació paquets clustering
install.packages(c("cluster", "factoextra", "clValid"))
```


En acabat, ens caldrà carregar les llibreries a la sessió R:

```{r message=FALSE}
# Carreguem les llibreries
library(cluster)
library(factoextra)
library(clValid)
```


[^1]: La documentació oficial es pot trobar a: http://www.sthda.com/english/rpkgs/factoextra.

## Preparació de les dades

D´entrada, per a realitzar una anàlisi d´agregació en R cal assegurar-se d´unes
quantes coses:

* Que les files es corresponen a observacions (individuals) i les columnes a variables.
* Qualsevol valor desconegut en el nostre conjunt de dades ha de ser o bé eliminat o bé substituït per exemple amb el valor de la mitjana o per el valor més freqüent.
*  Les dades han de ser estar discretitzades.


Per il·lustrar l´anàlisi d´agregació farem ús del conjunt de dades `USArrests`, que conté dades
estadístiques d´agressions, assassinats i violacions en cada un dels 50 estats d´USA l´any
1973.

```{r}
# Carreguem les dades
data("USArrests")
df <- USArrests
```

En primer lloc, podem eliminar els valors desconeguts en el nostre conjunt de dades
com es mostra a continuació:

```{r}
# Eliminem valors desconeguts
df <- na.omit(df)
```

En segon lloc, discretitzarem les nostres dades estandaritzant-les amb l´ajuda de
la funció `scale()`:

```{r}
# Estandaritzem les variables
df <- scale(df)
head(df, n = 3)
```



## Determinació del nombre de clústers

Per a determinar el nombre de clústers farem ús de la funció `fviz_nbclust()` del
paquet `factoextra` que calcula els mètodes __Elbow__, __Silhouhette__ i __Gap__.

El prototip de la funció es el següent:

```{r eval=FALSE}
fviz_nbclust(x, FUNcluster, method = c("silhouette", "wss", "gap_stat"))
```

on els arguments són els següents:

* **x:** matriu o data frame.
* **FUNcluster:** una funció d´agregació. Valors possibles: kmeans, pam, clara i hcut.
* **method:** mètode per a determinar el nombre òptim de clústers. Valors possibles:
 __Elbow__, __Silhouhette__ i __Gap__

A continuació, es mostra com determinar el nombre òptim de particions per al mètode
**_k-means_**:

```{r dpi=300}
# Mètode elbow
fviz_nbclust(df, kmeans, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2) +
  labs(x = "Nombre de particions k", 
       y = "Total intra-clúster suma de quadrats",
       title = "Nombre òptim de particions",
       subtitle = "Mètode Elbow") +
  theme_classic() +
  theme(plot.title = element_text(color="#4F81BD", size=14, face="bold"),
         plot.subtitle = element_text(color="#4F81BD", size=14))
  
       
  

# Mètode Silhouette
fviz_nbclust(df, kmeans, method = "silhouette") +
  labs(x = "Nombre de particions k", 
       y = "Ample de la mitjana de la silueta",
       title = "Nombre òptim de particions", 
       subtitle = "Mètode Silhouette") +
  theme_classic() +
  theme(plot.title = element_text(color="#4F81BD", size=14, face="bold"),
        plot.subtitle = element_text(color="#4F81BD", size=14))
  



# Mètode Gap
set.seed(123)
fviz_nbclust(df, kmeans, nstart = 25, 
             method = "gap_stat", nboot = 500) +
  labs(x = "Nombre de particions k", 
       y = "Valor de Gap (k)",
       title = "Nombre òptim de particions",
       subtitle = "Mètode Gap") +
  theme_classic() +
  theme(plot.title = element_text(color="#4F81BD", size=14, face="bold"),
        plot.subtitle = element_text(color="#4F81BD", size=14))
  
  
```

>Com podem observar en els gràfics:
>
* El mètode Elbow ens suggereix 4 clústers.
* El mètode Silhoutte ens suggereix 2 clústers.
* El mètode Gap ens sugereix 4 clústers.

Així és que, segons aquestes observacions podem considerar _k_ = 4 com el nombre
òptim de clústers.

## Mètode d´agregació _k-means_

A causa de que, l´algoritme _k-means_ comença seleccionant un centroide aleatòriament, es recomanable fer ús de la funció `set.seed()` a l´efecte de
aconseguir resultats reproduïbles. Així el lector d´aquest document obtindrà
els mateixos resultats que es presenten tot seguit.

A continuació es mostra com aplicar l´algorisme k-means amb k = 4:

```{r}
# Execució k-means amb k = 4
set.seed(123)
kmeansFit <- kmeans(df, 4, nstart = 25)
```

Podem mostrar per pantalla els resultats amb la següent línia de codi:

```{r}
# Mostrem els resultats
print(kmeansFit)
```

> Podem observar en la sortida el següent:
>
* La mitjana de clústers: una matriu, on les files són el nombre de clúster i
les columnes són les variables.
* El vector de particions: un vector d´enters (de 1:k) que indica el clúster on
cada observació ha sigut agrupada.

Així mateix, és recomanable realitzar un gràfic amb els resultats del model. Ja sigui, per a escollir el nombre de clústers, ja sigui per a comparar diferents anàlisis.

Una possible opció és visualitzar les dades en un diagrama de dispersió acolorint cada observació d'acord al grup assignat.

El problema és que el nostre conjunt de dades conté més de 2 variables i no és possible representar el model en dues dimensions.

Convé fer ressaltar que, una possible solució és reduir la dimensionalitat fent ús d´un algoritme de reducció del nombre d´atributs, com per exemple __Principal Component Analysis (PCA)__. 

En aquest sentit, farem ús de la funció `fviz_cluster()` que ens permetrà visualitzar els clústers i que utilitza PCA quan el nombre de variables és més gran de 2. Passarem com a arguments el resultat del model i el conjunt de dades original:

```{r dpi=300, fig.width=7, fig.height=7}
# Visualitzem els clústers
fviz_cluster(kmeansFit, data = df,
    main = "Gràfic de clústers",
    palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
    ellipse.type = "euclid", 
    star.plot = TRUE,
    repel = TRUE,
    ggtheme = theme_minimal()) + 
  theme(legend.position = "bottom",
        plot.title = element_text(color="#4F81BD", size=16, face="bold"))

```

Podem observar en el gràfic que les observacions són representades mitjançant punts i que en el nostre cas s´ha usat PCA. A més, s´han dibuixat el.lipses per tal  de
diferenciar cada clúster.

# Exercici 3

## Mètode d´agregació  _k-medoids_ 

El següent apartat tracta del mètode d´agregació __k-medoids__ i la seva implementació mitjançant l´algoritme de __Partició al voltant de Medoids PAM)__.

Igual com k-means, K-medoid és una tècnica clàssica de partició de grups que divideix les dades conformades per n objectes en k grups (amb k fixat _a priori_).

És més robust davant el soroll i valors atípics que k-means perquè minimitza una suma de dissimilituds (entre parells de punts) en comptes d'una suma de distàncies euclidiana quadrades.

Un __medoid__ pot ser definit com l'objecte d'un grup on la seva dissimilitud mitjana a tots els objectes en el grup és mínima. És el punt situat més cap al centre en tot el grup.[^2]

[^2]: Per a més informació: https://es.wikipedia.org/wiki/K-medoids

Per a determinar el nombre de clústers farem ús de la funció `pam()` del
paquet `cluster`.

El prototip de la funció es el següent:

```{r eval=FALSE}
pam(x, k, metric = "euclidean", stand = FALSE)
```

on els arguments són els següents:

* **x:** on x pot ser:
  + Una matriu o data frame de tipus numèric: cada fila correspon a una observació i cada columna a una variable.
  + Una matriu de dissimilituds: en aquest cas x es normalment la sortida o bé de la funció `daisy()` o bé de `dist()`.
* **K:** el nombre de clústers.
* **metric:** la mesura de similitud. O bé "euclidiana" o bé "manhattan".
* **stand:** un valor de tipus lògic; si es TRUE, les variables en x són estandarditzades.

### Estimació del nombre òptim de clùsters

Per a estimar el nombre òptim de clústers utilitzarem la mètrica de Silhoutte. La idea central és calcular l´algoritme PAM amb diferents valors de k. Per a la realització d´aquesta tasca farem ús de la funció `fviz_nbclust()`:

```{r dpi=300}

# Obtenció de k utilitzant la mètrica Silhoutte
fviz_nbclust(df, pam, method = "silhouette") +
  labs(x = "Nombre de particions k", 
       y = "Ample de la mitjana de la silueta",
       title = "Nombre òptim de particions", 
       subtitle = "Mètode Silhouette") +
  theme_classic() +
  theme(plot.title = element_text(color="#4F81BD", size=14, face="bold"),
        plot.subtitle = element_text(color="#4F81BD", size=14))
  
```

>Com podem observar en el gràfic el resultat per al mètode Silhoutte ens suggereix 2 clústers.



### Càlcul del mètode PAM

El següent codi calcula el mètode PAM amb k = 2:

```{r}
# Execució de l´algoritme PAM
pamFit <- pam(df, 2)
# Visualització de resultats
print(pamFit)
```

> Podem observar en la sortida el següent:
>
* Els grups medoids: una matriu, on les files són els medoids i les columnes són les variables.
* El vector de particions: un vector d´enters (de 1:k) que indica el clúster on
cada observació ha sigut agrupada.

Igual com hem fet en l´exercici anterior utilitzarem la funció `fviz_cluster()` del
paquet `factoextra` per a visualitzar les particions:

```{r dpi=300, fig.width=7, fig.height=7}
# Visualitzem els clústers
fviz_cluster(pamFit,
    main = "Gràfic de clústers",
    palette = c("#00AFBB", "#FC4E07"),
    ellipse.type = "t", 
    repel = TRUE,
    ggtheme = theme_minimal()) + 
  theme(legend.position = "bottom",
      plot.title = element_text(color="#4F81BD", size=16, face="bold"))
```



## El mètode aglomeradors _AGNES_

En el mètode de _k-means_ sempre es comença per un nombre fix de grups coneguts
_a priori_. Això fa que sigui útil quan tenim alguna idea de quants grups
hi ha en realitat.

Ara bé, hi ha situacions en què el nostre desconeixement del domini d’aplicació és encara més gran. Per tant, ni tan sols és possible fixar una quantitat _k_ de punts inicials entorn dels quals anar formant els grups.

En aquest cas, cal reflectir aquest desconeixement adoptant una actitud neutra respecte a les dades. Una de les maneres de resoldre el problema és mitjançant els _mètodes aglomeradors_.

> Els **mètodes aglomeradors** (en àngles, _Agglomerative Nesting_) comencen considerant que cada objecte forma un grup per si mateix (un grup d’un sol element) i llavors avaluen les distàncies entre grups (o objectes, en el primer pas) i creen per aglomeració els diversos grups finals.


### Mesures de similitud


Amb la finalitat de decidir quins objectes han de ser agrupats i quins clústers
dividits, hem de fer ús de les mesures de similitud entre els elements.

Tal com s´ha estudiat en l'apartat 3 dels apunts de l´assignatura existeixen
diferents mètodes per a calcular la di(similitud), com per exemple les distancies
Euclidià i _Hamming_.

Podem calcular les distancies entre cada par d´elements amb l´ajuda de la funció `dist():

```{r}
# Calcula la matriu de disimilitud
matrixDist <- dist(df, method = "euclidean")
```

El resultat de la línia de codi anterior és la matriu de distàncies o dissimilituds. Per defecte, la funció `dist()` calcula la distància Euclidià, no obstant podem
indicar altres mètriques passant-les a l'argument `method`.

### Càlcul de l´arbre jeràrquic

Així doncs, donada una matriu de distàncies calculada amb la funció `dist()`, podem
crear un arbre jeràrquic amb l´ajuda de la funció `hclust()`:

```{r}
hcFit <- hclust(d = matrixDist, method = "ward.D2")
```

Cal fer una especial referència, als diferents tipus de mètodes aglomeratius. Tot seguit, es mostren els més habituals:

>* Maximum o _complete linkage_: La distància entre dos clústers es defineix com
el màxim valor de totes les distàncies de cada par d´elements entre els elements
del clúster 1 i els elements del clúster 2. Aquest mètode tendeix a produir particions compactes.
>
* Minium o _single linkage_: La distància entre dos clústers es defineix com
el mínim valor de totes les distàncies de cada par d´elements entre els elements
del clúster 1 i els elements del clúster 2. Aquest mètode tendeix a produir particions molt grans.
>
* Mean o _average linkage_:  La distància entre dos clústers es defineix com la 
distància mitjana entre els elements de la partició 1 i els elements de la partició 2.
>
* *Centroid linkage*: La distància entre dos clústers es defineix com la distància entre el centroide de la particio 1 i el centroide de la partició 2.
>
* *Ward´s minium variance method*: Aquest mètode minimitza la variància intra-clústers. 

Convé destacar que, es recomanable utilitzar el mètode _Ward´s_ o _complete linkage_.

### Tallar el dendograma en diferents grups

Un dels problemes amb el clustering jeràrquic és que com no proporcionem el nombre
de clústers _a priori_, no sabem on tallar l´arbre per a formar clústers.

Podem fer ús de la funció `cutree()` per a tallar un arbre passant el nombre de
particions o l´altura de l´arbre:

```{r}
# Talla l´argre en 4 groups
grp <- cutree(hcFit, k = 4)
head(grp, n = 4)
```
 Podem conèixer el nombre d´elements en cada partició com es mostra a continuació:
 
```{r}
# Nombre de elements en cada partició
table(grp)
```
 
A continuació, es mostra la representació gràfica de l´arbre jeràrquic. Aquest
gràfic es coneix com a dendrograma:

```{r dpi=300, fig.width=7, fig.height=7}
# Talla en 4 groups i acoloreix per grups
fviz_dend(hcFit, k = 4,
  main = "Dendograma de clústers",
  horiz = TRUE,
  cex = 0.4, 
  k_colors = "jco",
  color_labels_by_k = TRUE, 
  rect = TRUE,
  rect_border = "jco",
  rect_fill = TRUE) +
  theme(legend.position = "bottom",
      plot.title = element_text(color="#4F81BD", size=16, face="bold"))
```

També, podem obtindre un dendrograma circular mitjançant la opció `type = "circular"`:

```{r dpi=300, fig.width=7, fig.height=7}
fviz_dend(hcFit, cex = 0.5, k = 4,
          k_colors = "jco", type = "circular")
```



Amb l´ajuda de la funció `fviz_cluster()`, podem visualitzar el resultat en un
diagrama de dispersió. Els elements són representats mitjançant punts, a més 
s´utilitza PCA:

```{r dpi=300, fig.width=7, fig.height=7}
# Diagrama de clústers
fviz_cluster(list(data = df, cluster = grp),
  main = "Gràfic de clústers",
  palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
  ellipse.type = "convex", 
  repel = TRUE, 
  show.clust.cent = FALSE, ggtheme = theme_minimal()) +
 theme(legend.position = "bottom",
    plot.title = element_text(color="#4F81BD", size=16, face="bold"))
```

## Comparació dels mètodes d´agrupació

Recollint tot el que s´ha dit, farem ús de la funció `clValid()` del paquet 
`clValid` per a comparar els diferents mètodes d´agrupació.

El prototip de la funció és el següent:

```{r eval=FALSE}
clValid(obj, nClust, clMethods = "hierarchical",
validation = "stability", maxitems = 600,
metric = "euclidean", method = "average")
```

on els arguments són:

* **obj**: Una matriu o data frame numèric. Les files son els elements que 
s´han d´agrupar i les columnes són les observacions.
* **nClust**: Un vector numèric especificant el nombre de particions que s´han
d´avaluar.
* **validation**: El tipus de mètrica de validació. Possibles valors són “internal”, “stability”, and “biological. També, permet escollir varies mètriques a la vegada.
* **maxitems**: El nombre màxim d´elements (files en la matriu) que han de ser
agrupades.
* **metric**: La mètrica utilitzada per a determinar la matriu de distàncies. Possibles valors són “euclidean”, “correlation”, i “manhattan”.
* **method**: Per a agrupacions jeràrquiques, el mètode d´aglomeració a utilitzar.
Les opcions disponibles són “ward”, “single”, “complete” i
“average”.

Per exemple, considerant el conjunt de dades `USArrests` que hem estat utilitzant per a il·lustrar els diferents mètodes d´aglomeració, podem utilitzar la funció `clValid()` com es mostra tot seguit per a calcular les diferents mètriques internes:

```{r}
# comparació dels mètodes d´agrupació
clmethods <- c("hierarchical", "kmeans", "pam")
intern <- clValid(df, nClust = 2:6,
          clMethods = clmethods, validation = "internal")
# Resúm
summary(intern)
```

> Com es pot observar el métode jerárquic amb dos clústers obté valors òptims en
les métriques _Connectivity_ i _Silhouette_ . Per altra banda, el mètode k-means
obté una bona puntuació en la mètrica _Dunn_ amb un valor òptim de _k_ = 6.




# Bibliografia



[1] Daniel T. Larouse, Chantal D. Larouse: Data Mininig and Predictive Analytics.USA, John Wiley & Sons,2015,ISBN 978-1-118-11619-7

[2] Jordi Gironés Roig, Jordi Casas Roma, Julià Minguillón Alfonso, Ramon Caihuelas Quiles : Minería de Datos: Modelos y Algoritmos. Barcelona, Editorial UOC, 2017, ISBN: 978-84-9116-904-8.

[3] Jiawe Han, Michellie Chamber & Jian Pei: Data mining : concepts and techniques. 3º Edition. USA, Editorial Elsevier, 2012, ISBN 978-0-12-381479-1









